{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generation with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "from music21 import converter, instrument, note, chord\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, Bidirectional\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files in the ./full_set_beethoven_mozart directory. Call BEFORE train \"\"\"\n",
    "    notes = []\n",
    "    durations = []\n",
    "    files = \"chopin/*.mid\"\n",
    "\n",
    "    for file in glob.glob(files):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "        print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi) #Change to only grab the piano???\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch) + \" \" +  str(element.quarterLength))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder) + \" \" + str(element.quarterLength))\n",
    "            elif isinstance(element, note.Rest):\n",
    "                notes.append(str(element.name)  + \" \" + str(element.quarterLength))\n",
    "\n",
    "    with open('data/notes.p', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 100\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "     # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "\n",
    "    network_output = to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(network_input, n_vocab):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    units = 512\n",
    "    dropout = 0.3\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                units,\n",
    "                dropout=dropout,\n",
    "                return_sequences=True\n",
    "            ),\n",
    "            input_shape=(network_input.shape[1], network_input.shape[2])\n",
    "        )\n",
    "    )\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(SeqSelfAttention(attention_activation='relu'))\n",
    "    model.add(LSTM(units, dropout=dropout))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(clipnorm=1.0))\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network():\n",
    "    \"\"\" Train a Neural Network to generate music \"\"\"\n",
    "    notes = get_notes()\n",
    "\n",
    "    n_vocab = len(set(notes))\n",
    "    \n",
    "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "    \n",
    "    model = create_network(network_input, n_vocab)\n",
    " \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'weights.hdf5',\n",
    "        monitor='loss',\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    # Your line of code here\n",
    "    model.fit(\n",
    "        x=network_input,\n",
    "        y=network_output,\n",
    "        batch_size=1024,\n",
    "        epochs=2000,\n",
    "        callbacks=callbacks_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing chopin/chp_op18_format0.mid\n",
      "Parsing chopin/chpn_op27_2.mid\n",
      "Parsing chopin/chpn_op25_e1.mid\n",
      "Parsing chopin/chpn-p19.mid\n",
      "Parsing chopin/chpn-p19_format0.mid\n",
      "Parsing chopin/chpn-p5.mid\n",
      "Parsing chopin/chpn_op25_e1_format0.mid\n",
      "Parsing chopin/chpn_op7_1.mid\n",
      "Parsing chopin/chpn_op27_1.mid\n",
      "Parsing chopin/chpn_op10_e01_format0.mid\n",
      "Parsing chopin/chpn-p20_format0.mid\n",
      "Parsing chopin/chpn_op25_e3.mid\n",
      "Parsing chopin/chpn_op10_e05_format0.mid\n",
      "Parsing chopin/chpn-p24_format0.mid\n",
      "Parsing chopin/chpn_op25_e12_format0.mid\n",
      "Parsing chopin/chpn-p24.mid\n",
      "Parsing chopin/chpn-p6_format0.mid\n",
      "Parsing chopin/chpn-p6.mid\n",
      "Parsing chopin/chpn-p9_format0.mid\n",
      "Parsing chopin/chpn_op33_2_format0.mid\n",
      "Parsing chopin/chpn-p2.mid\n",
      "Parsing chopin/chpn-p10.mid\n",
      "Parsing chopin/chp_op31.mid\n",
      "Parsing chopin/chpn-p7.mid\n",
      "Parsing chopin/chpn_op35_3_format0.mid\n",
      "Parsing chopin/chpn-p14_format0.mid\n",
      "Parsing chopin/chpn-p17_format0.mid\n",
      "Parsing chopin/chpn_op53_format0.mid\n",
      "Parsing chopin/chpn_op25_e3_format0.mid\n",
      "Parsing chopin/chpn-p17.mid\n",
      "Parsing chopin/chpn_op33_4.mid\n",
      "Parsing chopin/chpn_op7_2.mid\n",
      "Parsing chopin/chpn_op7_2_format0.mid\n",
      "Parsing chopin/chp_op31_format0.mid\n",
      "Parsing chopin/chpn-p3.mid\n",
      "Parsing chopin/chpn-p12_format0.mid\n",
      "Parsing chopin/chpn_op66_format0.mid\n",
      "Parsing chopin/chpn_op10_e01.mid\n",
      "Parsing chopin/chpn_op35_1_format0.mid\n",
      "Parsing chopin/chpn_op35_1.mid\n",
      "Parsing chopin/chpn-p10_format0.mid\n",
      "Parsing chopin/chpn_op25_e11.mid\n",
      "Parsing chopin/chpn-p8.mid\n",
      "Parsing chopin/chpn-p16.mid\n",
      "Parsing chopin/chpn-p4_format0.mid\n",
      "Parsing chopin/chpn_op35_4_format0.mid\n",
      "Parsing chopin/chpn_op7_1_format0.mid\n",
      "Parsing chopin/chpn-p23.mid\n",
      "Parsing chopin/chpn-p15_format0.mid\n",
      "Parsing chopin/chpn-p11.mid\n",
      "Parsing chopin/chpn-p1_format0.mid\n",
      "Parsing chopin/chpn-p21.mid\n",
      "Parsing chopin/chpn-p4.mid\n",
      "Parsing chopin/chpn_op25_e4.mid\n",
      "Parsing chopin/chpn-p18_format0.mid\n",
      "Parsing chopin/chpn_op35_3.mid\n",
      "Parsing chopin/chpn_op33_2.mid\n",
      "Parsing chopin/chpn-p21_format0.mid\n",
      "Parsing chopin/chpn-p3_format0.mid\n",
      "Parsing chopin/chpn_op35_4.mid\n",
      "Parsing chopin/chpn-p14.mid\n",
      "Parsing chopin/chpn-p23_format0.mid\n",
      "Parsing chopin/chpn-p15.mid\n",
      "Parsing chopin/chpn_op23_format0.mid\n",
      "Parsing chopin/chpn-p13.mid\n",
      "Parsing chopin/chpn_op25_e4_format0.mid\n",
      "Parsing chopin/chpn-p22.mid\n",
      "Parsing chopin/chpn_op10_e12.mid\n",
      "Parsing chopin/chpn-p9.mid\n",
      "Parsing chopin/chpn_op25_e11_format0.mid\n",
      "Parsing chopin/chpn-p8_format0.mid\n",
      "Parsing chopin/chpn-p12.mid\n",
      "Parsing chopin/chpn-p13_format0.mid\n",
      "Parsing chopin/chpn-p20.mid\n",
      "Parsing chopin/chpn-p11_format0.mid\n",
      "Parsing chopin/chpn_op10_e05.mid\n",
      "Parsing chopin/chpn_op25_e2.mid\n",
      "Parsing chopin/chpn_op33_4_format0.mid\n",
      "Parsing chopin/chpn_op25_e2_format0.mid\n",
      "Parsing chopin/chpn-p16_format0.mid\n",
      "Parsing chopin/chpn-p22_format0.mid\n",
      "Parsing chopin/chpn-p1.mid\n",
      "Parsing chopin/chpn-p2_format0.mid\n",
      "Parsing chopin/chpn_op27_2_format0.mid\n",
      "Parsing chopin/chpn_op35_2_format0.mid\n",
      "Parsing chopin/chpn-p7_format0.mid\n",
      "Parsing chopin/chpn-p18.mid\n",
      "Parsing chopin/chpn_op10_e12_format0.mid\n",
      "Parsing chopin/chpn_op66.mid\n",
      "Parsing chopin/chpn-p5_format0.mid\n",
      "Parsing chopin/chpn_op27_1_format0.mid\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 100, 1024)         2105344   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 1024)         0         \n",
      "_________________________________________________________________\n",
      "seq_self_attention (SeqSelfA (None, None, 1024)        65601     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 512)               3147776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2606)              1336878   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2606)              0         \n",
      "=================================================================\n",
      "Total params: 6,655,599\n",
      "Trainable params: 6,655,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "101/101 [==============================] - 69s 681ms/step - loss: 6.4720\n",
      "Epoch 2/2000\n",
      "101/101 [==============================] - 71s 703ms/step - loss: 6.2870\n",
      "Epoch 3/2000\n",
      "101/101 [==============================] - 72s 715ms/step - loss: 6.1806\n",
      "Epoch 4/2000\n",
      "101/101 [==============================] - 74s 732ms/step - loss: 6.0937\n",
      "Epoch 5/2000\n",
      "101/101 [==============================] - 74s 735ms/step - loss: 6.0567\n",
      "Epoch 6/2000\n",
      "101/101 [==============================] - 74s 732ms/step - loss: 6.0300\n",
      "Epoch 7/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 6.0134\n",
      "Epoch 8/2000\n",
      "101/101 [==============================] - 75s 744ms/step - loss: 5.9928\n",
      "Epoch 9/2000\n",
      "101/101 [==============================] - 75s 742ms/step - loss: 5.9799\n",
      "Epoch 10/2000\n",
      "101/101 [==============================] - 75s 741ms/step - loss: 5.9668\n",
      "Epoch 11/2000\n",
      "101/101 [==============================] - 76s 750ms/step - loss: 5.9508\n",
      "Epoch 12/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 5.9278\n",
      "Epoch 13/2000\n",
      "101/101 [==============================] - 75s 744ms/step - loss: 5.9067\n",
      "Epoch 14/2000\n",
      "101/101 [==============================] - 75s 741ms/step - loss: 5.8684\n",
      "Epoch 15/2000\n",
      "101/101 [==============================] - 76s 753ms/step - loss: 5.8235\n",
      "Epoch 16/2000\n",
      "101/101 [==============================] - 75s 740ms/step - loss: 5.7845\n",
      "Epoch 17/2000\n",
      "101/101 [==============================] - 76s 748ms/step - loss: 5.7400\n",
      "Epoch 18/2000\n",
      "101/101 [==============================] - 75s 742ms/step - loss: 5.6858\n",
      "Epoch 19/2000\n",
      "101/101 [==============================] - 76s 751ms/step - loss: 5.6288\n",
      "Epoch 20/2000\n",
      "101/101 [==============================] - 75s 742ms/step - loss: 5.5496\n",
      "Epoch 21/2000\n",
      "101/101 [==============================] - 76s 752ms/step - loss: 5.4837\n",
      "Epoch 22/2000\n",
      "101/101 [==============================] - 75s 745ms/step - loss: 5.4067\n",
      "Epoch 23/2000\n",
      "101/101 [==============================] - 75s 745ms/step - loss: 5.3405\n",
      "Epoch 24/2000\n",
      "101/101 [==============================] - 76s 756ms/step - loss: 5.2606\n",
      "Epoch 25/2000\n",
      "101/101 [==============================] - 76s 748ms/step - loss: 5.1836\n",
      "Epoch 26/2000\n",
      "101/101 [==============================] - 75s 742ms/step - loss: 5.1018\n",
      "Epoch 27/2000\n",
      "101/101 [==============================] - 75s 745ms/step - loss: 5.0389\n",
      "Epoch 28/2000\n",
      "101/101 [==============================] - 76s 748ms/step - loss: 4.9510\n",
      "Epoch 29/2000\n",
      "101/101 [==============================] - 77s 758ms/step - loss: 4.8749\n",
      "Epoch 30/2000\n",
      "101/101 [==============================] - 75s 746ms/step - loss: 4.7899\n",
      "Epoch 31/2000\n",
      "101/101 [==============================] - 75s 746ms/step - loss: 4.7089\n",
      "Epoch 32/2000\n",
      "101/101 [==============================] - 76s 748ms/step - loss: 4.6286\n",
      "Epoch 33/2000\n",
      "101/101 [==============================] - 75s 746ms/step - loss: 4.6402\n",
      "Epoch 34/2000\n",
      "101/101 [==============================] - 76s 752ms/step - loss: 4.4873\n",
      "Epoch 35/2000\n",
      "101/101 [==============================] - 75s 746ms/step - loss: 4.4094\n",
      "Epoch 36/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 4.3431\n",
      "Epoch 37/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 4.2751\n",
      "Epoch 38/2000\n",
      "101/101 [==============================] - 75s 745ms/step - loss: 4.2200\n",
      "Epoch 39/2000\n",
      "101/101 [==============================] - 76s 748ms/step - loss: 4.1381\n",
      "Epoch 40/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 4.0748\n",
      "Epoch 41/2000\n",
      "101/101 [==============================] - 75s 744ms/step - loss: 4.0050\n",
      "Epoch 42/2000\n",
      "101/101 [==============================] - 75s 744ms/step - loss: 3.9428\n",
      "Epoch 43/2000\n",
      "101/101 [==============================] - 77s 761ms/step - loss: 3.8829\n",
      "Epoch 44/2000\n",
      "101/101 [==============================] - 77s 758ms/step - loss: 3.8273\n",
      "Epoch 45/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 76s 748ms/step - loss: 3.7724\n",
      "Epoch 46/2000\n",
      "101/101 [==============================] - 75s 742ms/step - loss: 3.7098\n",
      "Epoch 47/2000\n",
      "101/101 [==============================] - 76s 754ms/step - loss: 3.6578\n",
      "Epoch 48/2000\n",
      "101/101 [==============================] - 75s 744ms/step - loss: 3.6058\n",
      "Epoch 49/2000\n",
      "101/101 [==============================] - 76s 756ms/step - loss: 3.5487\n",
      "Epoch 50/2000\n",
      "101/101 [==============================] - 75s 743ms/step - loss: 3.5003\n",
      "Epoch 51/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 3.4485\n",
      "Epoch 52/2000\n",
      "101/101 [==============================] - 76s 749ms/step - loss: 3.3930\n",
      "Epoch 53/2000\n",
      "101/101 [==============================] - 75s 744ms/step - loss: 3.3568\n",
      "Epoch 54/2000\n",
      "101/101 [==============================] - 75s 742ms/step - loss: 3.3013\n",
      "Epoch 55/2000\n",
      "101/101 [==============================] - 75s 745ms/step - loss: 3.2470\n",
      "Epoch 56/2000\n",
      "101/101 [==============================] - 75s 745ms/step - loss: 3.2146\n",
      "Epoch 57/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 3.1673\n",
      "Epoch 58/2000\n",
      "101/101 [==============================] - 76s 748ms/step - loss: 3.1221\n",
      "Epoch 59/2000\n",
      "101/101 [==============================] - 75s 741ms/step - loss: 3.0793\n",
      "Epoch 60/2000\n",
      "101/101 [==============================] - 76s 748ms/step - loss: 3.0439\n",
      "Epoch 61/2000\n",
      "101/101 [==============================] - 75s 744ms/step - loss: 3.0015\n",
      "Epoch 62/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 2.9691\n",
      "Epoch 63/2000\n",
      "101/101 [==============================] - 76s 749ms/step - loss: 2.9292\n",
      "Epoch 64/2000\n",
      "101/101 [==============================] - 75s 741ms/step - loss: 2.8983\n",
      "Epoch 65/2000\n",
      "101/101 [==============================] - 76s 749ms/step - loss: 2.8473\n",
      "Epoch 66/2000\n",
      "101/101 [==============================] - 75s 740ms/step - loss: 2.8152\n",
      "Epoch 67/2000\n",
      "101/101 [==============================] - 76s 757ms/step - loss: 2.7850\n",
      "Epoch 68/2000\n",
      "101/101 [==============================] - 75s 743ms/step - loss: 2.7558\n",
      "Epoch 69/2000\n",
      "101/101 [==============================] - 76s 748ms/step - loss: 2.7179\n",
      "Epoch 70/2000\n",
      "101/101 [==============================] - 75s 746ms/step - loss: 2.6889\n",
      "Epoch 71/2000\n",
      "101/101 [==============================] - 75s 745ms/step - loss: 2.6550\n",
      "Epoch 72/2000\n",
      "101/101 [==============================] - 75s 740ms/step - loss: 2.6337\n",
      "Epoch 73/2000\n",
      "101/101 [==============================] - 75s 742ms/step - loss: 2.5937\n",
      "Epoch 74/2000\n",
      "101/101 [==============================] - 75s 740ms/step - loss: 2.5663\n",
      "Epoch 75/2000\n",
      "101/101 [==============================] - 75s 746ms/step - loss: 2.5340\n",
      "Epoch 76/2000\n",
      "101/101 [==============================] - 75s 741ms/step - loss: 2.5079\n",
      "Epoch 77/2000\n",
      "101/101 [==============================] - 75s 742ms/step - loss: 2.4825\n",
      "Epoch 78/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 2.4612\n",
      "Epoch 79/2000\n",
      "101/101 [==============================] - 75s 746ms/step - loss: 2.4389\n",
      "Epoch 80/2000\n",
      "101/101 [==============================] - 76s 748ms/step - loss: 2.4230\n",
      "Epoch 81/2000\n",
      "101/101 [==============================] - 76s 748ms/step - loss: 2.3890\n",
      "Epoch 82/2000\n",
      "101/101 [==============================] - 76s 749ms/step - loss: 2.3649\n",
      "Epoch 83/2000\n",
      "101/101 [==============================] - 76s 755ms/step - loss: 2.3364\n",
      "Epoch 84/2000\n",
      "101/101 [==============================] - 75s 745ms/step - loss: 2.3637\n",
      "Epoch 85/2000\n",
      "101/101 [==============================] - 76s 749ms/step - loss: 2.3143\n",
      "Epoch 86/2000\n",
      "101/101 [==============================] - 75s 746ms/step - loss: 2.2713\n",
      "Epoch 87/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 2.2480\n",
      "Epoch 88/2000\n",
      "101/101 [==============================] - 76s 750ms/step - loss: 2.2326\n",
      "Epoch 89/2000\n",
      "101/101 [==============================] - 77s 763ms/step - loss: 2.2042\n",
      "Epoch 90/2000\n",
      "101/101 [==============================] - 76s 756ms/step - loss: 2.1889\n",
      "Epoch 91/2000\n",
      "101/101 [==============================] - 77s 761ms/step - loss: 2.1634\n",
      "Epoch 92/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 2.3725\n",
      "Epoch 93/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 2.1355\n",
      "Epoch 94/2000\n",
      "101/101 [==============================] - 76s 749ms/step - loss: 2.1162\n",
      "Epoch 95/2000\n",
      "101/101 [==============================] - 77s 759ms/step - loss: 2.0913\n",
      "Epoch 96/2000\n",
      "101/101 [==============================] - 76s 751ms/step - loss: 2.0685\n",
      "Epoch 97/2000\n",
      "101/101 [==============================] - 76s 750ms/step - loss: 2.0563\n",
      "Epoch 98/2000\n",
      "101/101 [==============================] - 76s 753ms/step - loss: 2.0570\n",
      "Epoch 99/2000\n",
      "101/101 [==============================] - 76s 756ms/step - loss: 2.0152\n",
      "Epoch 100/2000\n",
      "101/101 [==============================] - 75s 746ms/step - loss: 2.0064\n",
      "Epoch 101/2000\n",
      "101/101 [==============================] - 77s 763ms/step - loss: 1.9897\n",
      "Epoch 102/2000\n",
      "101/101 [==============================] - 77s 759ms/step - loss: 1.9725\n",
      "Epoch 103/2000\n",
      "101/101 [==============================] - 76s 751ms/step - loss: 3.3569\n",
      "Epoch 104/2000\n",
      "101/101 [==============================] - 76s 752ms/step - loss: 2.0052\n",
      "Epoch 105/2000\n",
      "101/101 [==============================] - 76s 754ms/step - loss: 1.9448\n",
      "Epoch 106/2000\n",
      "101/101 [==============================] - 76s 748ms/step - loss: 1.9257\n",
      "Epoch 107/2000\n",
      "101/101 [==============================] - 77s 761ms/step - loss: 1.9237\n",
      "Epoch 108/2000\n",
      "101/101 [==============================] - 77s 762ms/step - loss: 1.8902\n",
      "Epoch 109/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 1.8772\n",
      "Epoch 110/2000\n",
      "101/101 [==============================] - 76s 751ms/step - loss: 1.8737\n",
      "Epoch 111/2000\n",
      "101/101 [==============================] - 78s 777ms/step - loss: 1.8530\n",
      "Epoch 112/2000\n",
      "101/101 [==============================] - 76s 757ms/step - loss: 1.8334\n",
      "Epoch 113/2000\n",
      "101/101 [==============================] - 77s 764ms/step - loss: 1.8310\n",
      "Epoch 114/2000\n",
      "101/101 [==============================] - 76s 756ms/step - loss: 1.8042\n",
      "Epoch 115/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 1.8103\n",
      "Epoch 116/2000\n",
      "101/101 [==============================] - 75s 745ms/step - loss: 1.7861\n",
      "Epoch 117/2000\n",
      "101/101 [==============================] - 76s 756ms/step - loss: 1.9294\n",
      "Epoch 118/2000\n",
      "101/101 [==============================] - 76s 757ms/step - loss: 1.7689\n",
      "Epoch 119/2000\n",
      "101/101 [==============================] - 76s 757ms/step - loss: 1.7590\n",
      "Epoch 120/2000\n",
      "101/101 [==============================] - 77s 760ms/step - loss: 2.1751\n",
      "Epoch 121/2000\n",
      "101/101 [==============================] - 77s 758ms/step - loss: 1.7369\n",
      "Epoch 122/2000\n",
      "101/101 [==============================] - 77s 763ms/step - loss: 1.7358\n",
      "Epoch 123/2000\n",
      "101/101 [==============================] - 76s 752ms/step - loss: 1.7362\n",
      "Epoch 124/2000\n",
      "101/101 [==============================] - 77s 762ms/step - loss: 1.6960\n",
      "Epoch 125/2000\n",
      "101/101 [==============================] - 76s 755ms/step - loss: 1.8236\n",
      "Epoch 126/2000\n",
      "101/101 [==============================] - 76s 754ms/step - loss: 1.6766\n",
      "Epoch 127/2000\n",
      "101/101 [==============================] - 76s 754ms/step - loss: 1.6838\n",
      "Epoch 128/2000\n",
      "101/101 [==============================] - 76s 749ms/step - loss: 1.6513\n",
      "Epoch 129/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 1.6621\n",
      "Epoch 130/2000\n",
      "101/101 [==============================] - 76s 753ms/step - loss: 1.6295\n",
      "Epoch 131/2000\n",
      "101/101 [==============================] - 76s 749ms/step - loss: 1.6549\n",
      "Epoch 132/2000\n",
      "101/101 [==============================] - 77s 763ms/step - loss: 1.6238\n",
      "Epoch 133/2000\n",
      "101/101 [==============================] - 76s 751ms/step - loss: 1.6028\n",
      "Epoch 134/2000\n",
      "101/101 [==============================] - 76s 753ms/step - loss: 1.5972\n",
      "Epoch 135/2000\n",
      "101/101 [==============================] - 76s 752ms/step - loss: 1.5971\n",
      "Epoch 136/2000\n",
      "101/101 [==============================] - 76s 757ms/step - loss: 1.5835\n",
      "Epoch 137/2000\n",
      "101/101 [==============================] - 77s 763ms/step - loss: 1.5729\n",
      "Epoch 138/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 76s 749ms/step - loss: 1.6073\n",
      "Epoch 139/2000\n",
      "101/101 [==============================] - 76s 753ms/step - loss: 1.5974\n",
      "Epoch 140/2000\n",
      "101/101 [==============================] - 76s 755ms/step - loss: 2.4320\n",
      "Epoch 141/2000\n",
      "101/101 [==============================] - 76s 757ms/step - loss: 1.5572\n",
      "Epoch 142/2000\n",
      "101/101 [==============================] - 76s 754ms/step - loss: 1.5439\n",
      "Epoch 143/2000\n",
      "101/101 [==============================] - 77s 762ms/step - loss: 1.5276\n",
      "Epoch 144/2000\n",
      "101/101 [==============================] - 77s 758ms/step - loss: 1.5277\n",
      "Epoch 145/2000\n",
      "101/101 [==============================] - 77s 758ms/step - loss: 2.0269\n",
      "Epoch 146/2000\n",
      "101/101 [==============================] - 76s 751ms/step - loss: 1.5416\n",
      "Epoch 147/2000\n",
      "101/101 [==============================] - 77s 760ms/step - loss: 1.5000\n",
      "Epoch 148/2000\n",
      "101/101 [==============================] - 76s 753ms/step - loss: 1.5033\n",
      "Epoch 149/2000\n",
      "101/101 [==============================] - 77s 764ms/step - loss: 1.4848\n",
      "Epoch 150/2000\n",
      "101/101 [==============================] - 76s 757ms/step - loss: 1.5550\n",
      "Epoch 151/2000\n",
      "101/101 [==============================] - 76s 756ms/step - loss: 1.5023\n",
      "Epoch 152/2000\n",
      "101/101 [==============================] - 76s 757ms/step - loss: 1.4605\n",
      "Epoch 153/2000\n",
      "101/101 [==============================] - 76s 753ms/step - loss: 1.4642\n",
      "Epoch 154/2000\n",
      "101/101 [==============================] - 76s 756ms/step - loss: 4.0803\n",
      "Epoch 155/2000\n",
      "101/101 [==============================] - 78s 770ms/step - loss: 5.6288\n",
      "Epoch 156/2000\n",
      "101/101 [==============================] - 76s 753ms/step - loss: 3.8063\n",
      "Epoch 157/2000\n",
      "101/101 [==============================] - 76s 752ms/step - loss: 3.8569\n",
      "Epoch 158/2000\n",
      "101/101 [==============================] - 77s 760ms/step - loss: 3.0461\n",
      "Epoch 159/2000\n",
      "101/101 [==============================] - 77s 762ms/step - loss: 2.0512\n",
      "Epoch 160/2000\n",
      "101/101 [==============================] - 76s 752ms/step - loss: 1.9598\n",
      "Epoch 161/2000\n",
      "101/101 [==============================] - 76s 755ms/step - loss: 2.4717\n",
      "Epoch 162/2000\n",
      "101/101 [==============================] - 77s 760ms/step - loss: 2.2651\n",
      "Epoch 163/2000\n",
      "101/101 [==============================] - 77s 759ms/step - loss: 2.5372\n",
      "Epoch 164/2000\n",
      "101/101 [==============================] - 77s 763ms/step - loss: 2.1325\n",
      "Epoch 165/2000\n",
      "101/101 [==============================] - 78s 768ms/step - loss: 1.9240\n",
      "Epoch 166/2000\n",
      " 16/101 [===>..........................] - ETA: 1:05 - loss: 1.7976"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6fd1a7e30b9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-239ba94c3057>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Your line of code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
