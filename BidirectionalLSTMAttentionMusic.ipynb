{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generation with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "from music21 import converter, instrument, note, chord\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, Bidirectional\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files in the ./full_set_beethoven_mozart directory. Call BEFORE train \"\"\"\n",
    "    notes = []\n",
    "    durations = []\n",
    "    files = \"chopin/*.mid\"\n",
    "\n",
    "    for file in glob.glob(files):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "        print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi) #Change to only grab the piano???\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch) + \" \" +  str(element.quarterLength))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder) + \" \" + str(element.quarterLength))\n",
    "            elif isinstance(element, note.Rest):\n",
    "                notes.append(str(element.name)  + \" \" + str(element.quarterLength))\n",
    "\n",
    "    pickle.dump(notes, open('data/notes.p', 'wb'))\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 100\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "     # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "\n",
    "    network_output = to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(network_input, n_vocab):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    units = 512\n",
    "    dropout = 0.3\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                units,\n",
    "                dropout=dropout,\n",
    "                return_sequences=True\n",
    "            ),\n",
    "            input_shape=(network_input.shape[1], network_input.shape[2])\n",
    "        )\n",
    "    )\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(SeqSelfAttention(attention_activation='relu'))\n",
    "    model.add(LSTM(units, dropout=dropout))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(clipnorm=1.0))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(load_notes=False, to_load_model=False, learning_rate=None):\n",
    "    \"\"\" Train a Neural Network to generate music \"\"\"\n",
    "    notes = None\n",
    "    model = None\n",
    "\n",
    "    if load_notes:\n",
    "        notes = pickle.load(open('data/notes.p', 'rb'))\n",
    "    else:\n",
    "        notes = get_notes()\n",
    "    \n",
    "    n_vocab = len(set(notes))\n",
    "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "\n",
    "    if load_model:\n",
    "        model = load_model('weights.hdf5', custom_objects={'SeqSelfAttention': SeqSelfAttention})\n",
    "    else:\n",
    "        model = create_network(network_input, n_vocab)\n",
    "\n",
    "    if learning_rate != None:\n",
    "        K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'weights.hdf5',\n",
    "        monitor='loss',\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    model.fit(\n",
    "        x=network_input,\n",
    "        y=network_output,\n",
    "        batch_size=1024,\n",
    "        epochs=2000,\n",
    "        callbacks=callbacks_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 100, 1024)         2105344   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 1024)         0         \n",
      "_________________________________________________________________\n",
      "seq_self_attention (SeqSelfA (None, None, 1024)        65601     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 512)               3147776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2606)              1336878   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2606)              0         \n",
      "=================================================================\n",
      "Total params: 6,655,599\n",
      "Trainable params: 6,655,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "101/101 [==============================] - 70s 691ms/step - loss: 1.3167\n",
      "Epoch 2/2000\n",
      "101/101 [==============================] - 72s 709ms/step - loss: 1.2894\n",
      "Epoch 3/2000\n",
      "101/101 [==============================] - 72s 710ms/step - loss: 1.2573\n",
      "Epoch 4/2000\n",
      "101/101 [==============================] - 72s 716ms/step - loss: 1.2438\n",
      "Epoch 5/2000\n",
      "101/101 [==============================] - 74s 729ms/step - loss: 1.2385\n",
      "Epoch 6/2000\n",
      "101/101 [==============================] - 74s 734ms/step - loss: 1.2311\n",
      "Epoch 7/2000\n",
      "101/101 [==============================] - 74s 737ms/step - loss: 1.2284\n",
      "Epoch 8/2000\n",
      "101/101 [==============================] - 75s 739ms/step - loss: 1.2147\n",
      "Epoch 9/2000\n",
      "101/101 [==============================] - 75s 740ms/step - loss: 1.2139\n",
      "Epoch 10/2000\n",
      "101/101 [==============================] - 75s 741ms/step - loss: 1.2083\n",
      "Epoch 11/2000\n",
      "101/101 [==============================] - 75s 743ms/step - loss: 1.1940\n",
      "Epoch 12/2000\n",
      "101/101 [==============================] - 76s 748ms/step - loss: 1.1921\n",
      "Epoch 13/2000\n",
      "101/101 [==============================] - 75s 746ms/step - loss: 1.1903\n",
      "Epoch 14/2000\n",
      "101/101 [==============================] - 75s 744ms/step - loss: 1.1882\n",
      "Epoch 15/2000\n",
      "101/101 [==============================] - 75s 744ms/step - loss: 1.1833\n",
      "Epoch 16/2000\n",
      "101/101 [==============================] - 75s 747ms/step - loss: 1.1706\n",
      "Epoch 17/2000\n",
      "101/101 [==============================] - 75s 741ms/step - loss: 1.1740\n",
      "Epoch 18/2000\n",
      "101/101 [==============================] - 76s 749ms/step - loss: 1.1613\n",
      "Epoch 19/2000\n",
      "101/101 [==============================] - 76s 749ms/step - loss: 1.1629\n",
      "Epoch 20/2000\n",
      "101/101 [==============================] - 75s 745ms/step - loss: 1.1557\n",
      "Epoch 21/2000\n",
      "101/101 [==============================] - 76s 749ms/step - loss: 1.1541\n",
      "Epoch 22/2000\n",
      "101/101 [==============================] - 75s 743ms/step - loss: 1.1542\n",
      "Epoch 23/2000\n",
      "101/101 [==============================] - 75s 745ms/step - loss: 1.1444\n",
      "Epoch 24/2000\n",
      "101/101 [==============================] - 76s 750ms/step - loss: 1.1447\n",
      "Epoch 25/2000\n",
      "101/101 [==============================] - 76s 752ms/step - loss: 1.1396\n",
      "Epoch 26/2000\n",
      "101/101 [==============================] - 76s 754ms/step - loss: 1.1332\n",
      "Epoch 27/2000\n",
      "101/101 [==============================] - 75s 739ms/step - loss: 1.1346\n",
      "Epoch 28/2000\n",
      "101/101 [==============================] - 75s 742ms/step - loss: 1.1258\n",
      "Epoch 29/2000\n",
      "101/101 [==============================] - 75s 741ms/step - loss: 1.1264\n",
      "Epoch 30/2000\n",
      "101/101 [==============================] - 75s 748ms/step - loss: 1.1156\n",
      "Epoch 31/2000\n",
      "101/101 [==============================] - 75s 743ms/step - loss: 1.1144\n",
      "Epoch 32/2000\n",
      "101/101 [==============================] - 76s 751ms/step - loss: 1.1146\n",
      "Epoch 33/2000\n",
      "101/101 [==============================] - 75s 745ms/step - loss: 1.1069\n",
      "Epoch 34/2000\n",
      "101/101 [==============================] - 76s 752ms/step - loss: 1.1036\n",
      "Epoch 35/2000\n",
      "101/101 [==============================] - 75s 743ms/step - loss: 1.0993\n",
      "Epoch 36/2000\n",
      "101/101 [==============================] - 76s 752ms/step - loss: 1.1042\n",
      "Epoch 37/2000\n",
      "101/101 [==============================] - 75s 743ms/step - loss: 1.0956\n",
      "Epoch 38/2000\n",
      "101/101 [==============================] - 77s 764ms/step - loss: 1.0976\n",
      "Epoch 39/2000\n",
      "101/101 [==============================] - 76s 753ms/step - loss: 1.0882\n",
      "Epoch 40/2000\n",
      "101/101 [==============================] - 76s 755ms/step - loss: 1.0856\n",
      "Epoch 41/2000\n",
      "101/101 [==============================] - 76s 748ms/step - loss: 1.0861\n",
      "Epoch 42/2000\n",
      "101/101 [==============================] - 76s 749ms/step - loss: 1.0867\n",
      "Epoch 43/2000\n",
      "  8/101 [=>............................] - ETA: 1:03 - loss: 1.0649"
     ]
    }
   ],
   "source": [
    "train_network(True, True, 0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
